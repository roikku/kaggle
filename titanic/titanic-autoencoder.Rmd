---
title: 'Diabolo Trick: Pre-train, Train and Solve'
author: 'Loic Merckel'
date: '24 June 2017'
output:
  html_document:
    number_sections: false
    toc: true
    highlight: tango
    theme: cosmo
    smart: true
---

<style type="text/css">
  h1.title { font-weight: bold; } h1 { font-weight: normal; } .author { font-weight: normal; font-size: 1.5em; }
</style>


```{r include=FALSE}
# License -----------------

# Copyright 2017 Loic Merckel
# 
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
# http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```


This page derived from the blog post [*Kaggle's Titanic Toy Problem Revisited*](https://goo.gl/E6F4bm).

Throughout this post we present another[^1] way to address the Kaggle's *toy* problem *Titanic: Machine Learning from Disaster*. In particular, we show how to:

* Leverage unlabeled data in a supervised classification problem (a technique often referred to as **semi-supervised learning**).
* Use the autoencoder and deeplearning capability of the wonderful [H<small>2</small>O](https://www.h2o.ai/) framework.
 
[^1]: We recently posted a solution, [Random Forest, Forward Feature Selection & Grid Search](https://goo.gl/CqfMg4)

Note that the method presented in this page does not propose a model that can make predictions from new data (unlike our previous approach [here](https://goo.gl/CqfMg4)). The test set is extensively used during the process; it can be regarded as a label reconstruction method.

Finally, we observe fairly good results (e.g., 0.80383 on the Kaggle's leadderboard with this source code); which is quite remarkable given the ridiculously small size of the data set (a setting usually prone to favor tree-based methods---or generalized linear model if the transformed response by the link function varies linearly with the features---over neural networks).


```{r include=FALSE}
pkgs <- c("h2o", "parallel", 
          "corrplot", "car", "heplots", "plyr",
          "stringr", "caret", "dplyr", "gridExtra") 
for (pkg in pkgs) {
  if (! (pkg %in% rownames(installed.packages()))) { install.packages(pkg) }
  suppressPackageStartupMessages(require(pkg, character.only = TRUE))
}
rm(pkgs)
rm(pkg)

h2o.init(nthreads = parallel:::detectCores(), 
         max_mem_size = "6g", min_mem_size = "1g")
h2o.removeAll()
```

```{r include=FALSE}
# data ---------------------------

rm(list=ls(all=TRUE))

getDataSet <- function (filePath) {
  if(!file.exists(filePath)){
    stop ("Error: the file cannot be found...")
  }
  return (read.csv(filePath, header = TRUE))
}

isOnKaggle <- FALSE
if (isOnKaggle) {
  X <- getDataSet (file.path("..", "input", "train.csv", fsep = .Platform$file.sep))
  Xt <- getDataSet(file.path("..", "input", "test.csv", fsep = .Platform$file.sep))
} else {
  X <- getDataSet (file.path(".", "data", "train.csv", fsep = .Platform$file.sep))
  Xt <- getDataSet(file.path(".", "data", "test.csv", fsep = .Platform$file.sep))
}

# convert the column-to-predict to factor
X[["Survived"]] <- as.factor (X[["Survived"]])  

Xt$Survived <- NA
levels(Xt$Survived) <- as.numeric(levels(X$Survived))

# move the response to the end
X <- X[,c(setdiff(names(X),"Survived"), "Survived")]
Xt <- Xt[,c(setdiff(names(Xt),"Survived"), "Survived")]
```

```{r include=FALSE}
kSplitRatios <- c(0.95)  

# Functions & Others  -----------------------------

saveOutPutCsv <- function (id, pred, file) {
  res <- data.frame(PassengerId=id)
  res[["Survived"]] <- as.numeric(as.vector(pred))
  write.csv(res, file=file, row.names=FALSE,  quote = FALSE)
}

showBarPlot <- function (feature, y="Survived", data = X, legendPos = "topleft", stack = FALSE, normalize = FALSE, labels = c("Died", "Survived")) {
  tb <- table(data[[y]], data[[feature]])
  if (normalize) {
    for (k in 1:dim(tb)[2]) {
      m <- sum(tb[, k])
      tb[1, k] <- tb[1, k] * 100 / m
      tb[2, k] <- tb[2, k] * 100 / m
    }
  }
  barplot(tb,  beside = !stack, legend = FALSE, 
          xlab=feature, ylab="", col=c("firebrick", "darkgreen"))
  if (length(labels) > 0) {
    legend(legendPos, 
           legend = labels, 
           fill = c("firebrick", "darkgreen"))
  }
  return (tb)
}


factor2Numeric <- function (f) {
  return (as.numeric(levels(f))[f])
}

```

# Features Tinkering

## Feature Preparation

The features are prepared the exact same way as described in our previous post (*kernels*) [Random Forest, Forward Feature Selection & Grid Search](https://goo.gl/CqfMg4).

```{r  include=FALSE}

set.seed(18746635)
toRm <- c()

#xxxxxxxxxxxxxxxxxxxxxxxx
# X$Sex

addSexNum <- function(X) {
  X$SexNum[X$Sex == "male"] <- 1
  X$SexNum[X$Sex != "male"] <- 0
  
  return (X)
}

X <- addSexNum (X)
Xt <- addSexNum (Xt)

showBarPlot("SexNum")

toRm <- c(toRm, "Sex")


#xxxxxxxxxxxxxxxxxxxxxxxx
# X$Ticket

addTicketNum <- function(X) {
  X$TicketNum <- sapply(X$Ticket, function (x) {
    str <- as.character(x)
    s <- unlist(strsplit(str, " "))
    if (length(s) > 1) {
      str <- s[2]
    }
    ret <- tryCatch({
      ret <- ifelse(length(str) == 0, 0, as.numeric(str))
    }, warning=function(cond) {
      print (str)
      ret <- -1
    }
    )
    return (ret)
  })
  return(X)
}

X <- addTicketNum (X)
Xt <- addTicketNum (Xt)

# check for outliers
max(X$TicketNum)
#hist(X$TicketNum, breaks = 50)
# there are a couple of outliers...
# naive data imputation... could be revised for improvment
X$TicketNum <- sapply(X$TicketNum, function (x) {
  return (ifelse(x > 1500000, median(X$TicketNum), x))
})
Xt$TicketNum <- sapply(Xt$TicketNum, function (x) {
  return (ifelse(x > 1500000, median(X$TicketNum), x))
})
#hist(X$TicketNum, breaks = 50)

boxplot(TicketNum ~ Survived, data = X,  
        xlab = "Survived", ylab = "TicketNum", 
        grid = FALSE, col = c("firebrick", "darkgreen"))

toRm <- c(toRm, "Ticket")

#xxxxxxxxxxxxxxxxxxxxxxxx
# X$Cabin

# let's extract the letter
addCabinChar <- function(X) {
  X$CabinChar <- sapply(X$Cabin, function (x) {
    str <- as.character(x)
    s <- unlist(strsplit(str, " "))
    if (length(s) == 1) {
      str <- substr(s, 1, 1)
    } else if (length(s) == 0) {
      str <- "X"
    }else {
      # the list seems to be all with the same letter
      str <- substr(s[1], 1, 1)
    }
    return (str)
  })
  X$CabinChar <- as.factor(X$CabinChar)
  return(X)
}

X <- addCabinChar (X)
Xt <- addCabinChar (Xt)

# there are more factors in X than Xt
Xt$CabinChar <- factor(Xt$CabinChar, levels=c(levels(X$CabinChar)))

# Let's make a new feature
addCabinKnown <- function(X) {
  X$CabinKnown[X$CabinChar == "X"] <- 0
  X$CabinKnown[X$CabinChar != "X"] <- 1
  
  return (X)
}

X <- addCabinKnown (X)
Xt <- addCabinKnown (Xt)

showBarPlot("CabinKnown", stack=TRUE, normalize = TRUE, legendPos = "bottomright")

toRm <- c(toRm, "Cabin")

#xxxxxxxxxxxxxxxxxxxxxxxx
# X$Embarked

sum (X$Embarked == "")
sum (Xt$Embarked == "")
# only two passengers embarked from unknown locations

sum (Xt$Embarked == "")
# none on the test set

# get the two rows in questions
X[X$Embarked == "", ]
# two first class women that survived, but it seems they have the 
# same ticket number... same cabin... same fare... Weird!

# Even though the data set is very small, 
# we could remove those two outliers...
X <- X[-which(X$Embarked == ""), ]
X$Embarked <- factor(X$Embarked) # drop the empty level

# there seems to be two categories here...
addEmbarkedQorS <- function(X) {
  X$EmbarkedQorS[X$Embarked %in% c("Q", "S")] <- 1
  X$EmbarkedQorS[!(X$Embarked %in% c("Q", "S"))] <- 0
  
  return (X)
}

X <- addEmbarkedQorS (X)
Xt <- addEmbarkedQorS (Xt)

showBarPlot("EmbarkedQorS", stack=TRUE, normalize = TRUE, legendPos = "bottomright")

#xxxxxxxxxxxxxxxxxxxxxxxx
# X$Fare & X$Age

# Age has a lot of missing values, 
# and Fare has one missing value in the test set.

# We impute missing ages and fare

# First, get rid of the features we no longer need, for we will impute
# missing ages and fare using the useful features
getIndexToIgnore <- function (X, toRm) {
  indexToIgnore <- c()
  for (feature in toRm) {
    ind <- grep(paste0("^", feature, "$"), colnames(X))
    if (!identical(ind, integer(0))) {
      indexToIgnore <- c(indexToIgnore, ind)
    }
  }
  
  # Index of other columns to be ignored
  pi <- grep("^PassengerId$", colnames(X))
  if (!identical(pi, integer(0))) {
    indexToIgnore <- c(indexToIgnore, pi)
  }
  su <- grep("^Survived$", colnames(X))
  if (!identical(su, integer(0))) {
    indexToIgnore <- c(indexToIgnore, su)
  }
  return(indexToIgnore)
}

indexToIgnore <- getIndexToIgnore(X, toRm)

set.seed(18746635)
preProcX <- preProcess(X[, -indexToIgnore], 
                       method = c("knnImpute", "center", "scale"), 
                       k = 8) 

plotHist <- function (X, original, reconstructed, hor = TRUE) {
  xmin <- min(0)
  xmax <- max(length(as.vector(X[[reconstructed]])), na.rm = TRUE)
  ymin <- min(X[[reconstructed]], na.rm = TRUE)
  ymax <- max(X[[reconstructed]], na.rm = TRUE)
  
  if (hor) {
    mfrow <- c(2, 1)
  } else {
    mfrow <- c(1, 2)
  }
  opar <- par(mfrow = mfrow)
  hist(as.vector(X[[reconstructed]]), breaks = 100, main=original, 
       xlim = c(ymin, ymax), xlab = "Reconstructed")
  hist(as.vector(X[[original]]), breaks = 100, main=original, 
       xlim = c(ymin, ymax), xlab = paste0("Original (", format(sum(is.na(X[[original]])) / dim(X)[1] * 100, digits = 3), "% of missing values)"))
  par(opar)
  return(0)
}

predictedMissingValues <- predict(preProcX, X[, -indexToIgnore]) 
predictedMissingValuesTest <- predict(preProcX, Xt[, -indexToIgnore]) 

# Age
XpredAge <- predictedMissingValues[["Age"]]
XpredAget <- predictedMissingValuesTest[["Age"]]

# we scale back the feature, because we might need the Age
# for tinkering other features
meanAge <- mean(X$Age, na.rm = TRUE)
sdAge <- sd(X$Age, na.rm = TRUE)

X$AgePred <- (XpredAge * sdAge) + meanAge
Xt$AgePred <- (XpredAget * sdAge) + meanAge

plotHist (X, "Age", "AgePred", hor = FALSE)

# Fare
XpredFare <- X$Fare
XpredFaret <- predictedMissingValuesTest[["Fare"]]

meanFare <- mean(X$Fare, na.rm = TRUE)
sdFare <- sd(X$Fare, na.rm = TRUE)

XpredFaret <- (XpredFaret * sdFare) + meanFare

X$FarePred <- XpredFare
Xt$FarePred <- XpredFaret

opar <- par(mar=c(5, 5.2, 4, 2) + 0.1)
boxplot(FarePred ~ Survived, data = X,  
        xlab = "Survived", ylab = "FarePred\n(outliers removed for visibility)", 
        grid = FALSE, col = c("firebrick", "darkgreen"), outline=FALSE)
par(opar)

toRm <- c(toRm, "Fare", "Age")

# observe what is happening with age ranges
addAgeRange <- function(X) {
  
  X$AgeRange[X$AgePred <= 1] <- 0
  X$AgeRange[X$AgePred > 1 & X$AgePred <= 8] <- 2
  X$AgeRange[X$AgePred > 8] <- 3
  X$AgeRange[is.na(X$AgePred)] <- 7
  
  X$AgeRange <- as.numeric(X$AgeRange)
  
  return (X)
}

X <- addAgeRange (X)
Xt <- addAgeRange (Xt)

# the fact that the age is known might tell us something
addAgeKnown <- function(X) {
  X$AgeKnown <- 1
  X$AgeKnown[is.na(X$Age)] <- 0
  return(X)
}

X <- addAgeKnown (X)
Xt <- addAgeKnown (Xt)

#xxxxxxxxxxxxxxxxxxxxxxxx
# X$Name

addTitle <- function(X) {
  X$Title <- sapply(X$Name, function (x) {
    str <- as.character(x)
    title <- str_extract(str, ", ([A-Za-z]+)\\.")
    title <- substr(title, nchar(", ") + 1, nchar(title) - 1)
    return (title)
  })
  # some translations
  X$Title[X$Title %in% c("Don", "Rev", "Major", "Sir", "Col", "Capt", "Jonkheer")] <- "Mr"
  X$Title[X$Title %in% c("the Countess", "Mlle", "Ms")] <- "Miss"
  X$Title[X$Title %in% c("Mme", "Lady", "Dona")] <- "Mrs"
  
  X$Title[is.na(X$Title) & X$Sex == "male"] <- "Mr"
  X$Title[is.na(X$Title) & X$Sex == "female" & X$SibSp == 0] <- "Miss"
  X$Title[is.na(X$Title) & X$Sex == "female" & X$SibSp > 0] <- "Mrs" # assuming it is not a sibling...
  
  # in 1912, most doctors were men...
  X$Title[X$Title %in% c("Dr") & X$Sex == "male"] <- "Mr"
  X$Title[X$Title %in% c("Dr") & X$Sex == "female" & X$SibSp == 0] <- "Miss"
  X$Title[X$Title %in% c("Dr") & X$Sex == "female" & X$SibSp > 0] <- "Mrs" # assuming it is not a sibling...
  
  # Master seems to be boys only, we add girls
  #X$Title[X$Title %in% c("Miss") & X$AgePred < 10] <- "Child"
  X$Title[X$Title %in% c("Master")] <- "Child"
  X$Title[X$AgePred < 10] <- "Child"
  #X$Title[X$AgePred > 10 & X$AgePred < 18] <- "Teenage"
  
  X$Title <- as.factor(X$Title)
  return(X)
}

X <- addTitle (X)
Xt <- addTitle (Xt)

toRm <- c(toRm, "Name")

# redundant with Title, let the feature selector pick
addGender <- function(X) {
  X$Gender[(X$Title %in% c("Miss", "Mrs"))] <- "Woman"
  X$Gender[(X$Title %in% c("Mr"))] <- "Man"
  X$Gender[(X$Title %in% c("Child"))] <- "Child"
  return(X)
}

X <- addGender (X)
Xt <- addGender (Xt)

opar <- par(mfrow = c(1, 2))
showBarPlot("Title", stack=TRUE, normalize = TRUE, legendPos = "topleft")
showBarPlot("Gender", stack=TRUE, normalize = TRUE, legendPos = "topleft")
par(opar)

#xxxxxxxxxxxxxxxxxxxxxxxx
# X$Parch & X$SibSp

# we add some features based on those two columns

# Size of families
addFamilyMembers <- function(X) {
  X$FamilyMembers <- X$Parch + X$SibSp
  return (X)
}

X <- addFamilyMembers (X)
Xt <- addFamilyMembers (Xt)

# large families seem to be doomed
addLargeFamily <- function(X) {
  X$LargeFamily <- X$Parch + X$SibSp
  
  # bad coding! here order matters... to refactor
  X$LargeFamily[X$LargeFamily < 7] <- 0
  X$LargeFamily[X$LargeFamily >= 7] <- 1
  
  X$LargeFamily <- as.factor(X$LargeFamily)
  
  return (X)
}

X <- addLargeFamily (X)
Xt <- addLargeFamily (Xt)

# is the passenger alone
addSingle <- function(X) {
  X$Single <- 0
  X$Single[which(X$Parch == 0 & X$SibSp == 0)] <- 1
  return (X)
}

X <- addSingle (X)
Xt <- addSingle (Xt)

# number of children with a woman passenger
addMotherChildNumber <- function(X) {
  X$MotherChildNumber <- 0
  X$MotherChildNumber[which(X$Parch %in% c(1, 2, 3) & X$AgePred > 20 & X$Title %in% c("Mrs", "Miss"))] <- 1
  X$MotherChildNumber[which(X$Parch > 3 & X$AgePred > 20 & X$Title %in% c("Mrs", "Miss"))] <- 2
  return (X)
}

X <- addMotherChildNumber (X)
Xt <- addMotherChildNumber (Xt)

opar <- par(mfrow = c(1, 3))
showBarPlot("FamilyMembers", stack=TRUE, normalize = TRUE, legendPos = "bottomright")
showBarPlot("Single", stack=TRUE, normalize = TRUE, legendPos = "bottomright")
showBarPlot("MotherChildNumber", stack=TRUE, normalize = TRUE, legendPos = "bottomright")
par(opar)

# remove some columns we think we no longer need
for (feature in toRm) {
  X[[feature]] <- NULL
  Xt[[feature]] <- NULL
}
```

The figure below presents some charts relative to the six features we will select (see section [Feature Selection](#feature-selection)).

```{r echo=FALSE, warning=FALSE, results='hide', message=FALSE, fig.width=10, fig.height=5.0}
opar <- par(mfrow = c(2, 3))
showBarPlot("Title", stack=TRUE, normalize = TRUE, legendPos = "topleft")
showBarPlot("EmbarkedQorS", stack=TRUE, normalize = TRUE, legendPos = "bottomright")
showBarPlot("Pclass", stack=TRUE, normalize = TRUE, legendPos = "bottomright")
showBarPlot("FamilyMembers", stack=TRUE, normalize = TRUE, legendPos = "bottomright")

boxplot(TicketNum ~ Survived, data = X,  
        xlab = "Survived", ylab = "TicketNum", 
        grid = FALSE, col = c("firebrick", "darkgreen"))

oparnested <- par(mar=c(5, 5.2, 4, 2) + 0.1)
boxplot(FarePred ~ Survived, data = X,  
        xlab = "Survived", ylab = "FarePred\n(outliers removed for visibility)", 
        grid = FALSE, col = c("firebrick", "darkgreen"), outline=FALSE)
par(oparnested)
par(opar)
```

## Dealing With Categorical Variables

Unlike our previous approach relying on tree-based learning, here we use neural networks, and thus we need to encode categorical variables. The H<small>2</small>O framework offers, out-of-the-box, several of the popular techniques to achieve such an encoding: one-hot, binary, enumeration (label)---plus some other methods, see [here](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/algo-params/categorical_encoding.html) for a full description.

*Enum* encoding is often a poor choice, unless there exist a meaningful order relationship between the categories. If ordering categories does not make sense---e.g., the *Title* feature consists of four categories: Child, Miss, Mr and Mrs---then other encodings might constitute wiser options.

*Binary* encoding makes sense when the number of categories is large; it typically adds 32 columns per categorical variable---and therefore one-hot encoding appears the right choice when the number of categories is less than 32.

In our selected subset of features (see section [Feature Selection](#feature-selection)), *Title* is the one with the highest number of categories (four only), and therefore we use one-hot encoding (in H<small>2</small>O, the parameter `categorical_encoding` needs to be either set to `OneHotInternal` or left to `AUTO`, for one-hot is the default option).


# Data Split 

```{r include=FALSE}
unselect <- c("PassengerId")
response <- "Survived"

# H2o is buggy regarding conversion of NA columns...
Xt$Survived <- NA
Xt$Survived[1] <- 1
Xt$Survived[2] <- 0
test <- as.h2o(Xt)
test[, response] <- h2o.asfactor(test[, response])

train <- as.h2o(X)

predictors <- setdiff(names(train), c(response, unselect)) 
```

The entire data set is used for training autoencoders (unsupervised learning).

```{r echo=TRUE, warning=FALSE, results='hide', message=FALSE}
trainUnsupervised <- h2o.rbind(train, test) 
```

Regarding the deep learning classifier, a validation set is extracted from the train set. We use a validation set (i) to pick an adequate threshold, and subsequently (ii) to rank the different models (obtained from the autoencoder grid search). 

```{r echo=TRUE, warning=FALSE, results='hide', message=FALSE}
splits <- h2o.splitFrame(
  data = train, 
  ratios = kSplitRatios,  
  seed = 1234
)
trainSupervised <- splits[[1]]
validSupervised <- splits[[2]]
```



# Feature selection 

We use the following features: *Title*, *FarePred*, *EmbarkedQorS*, *TicketNum*, *FamilyMembers*, *Pclass*. 

This subset of features has been obtained using a backwards version of the stepwise feature selection presented in our previous post ([Random Forest, Forward Feature Selection & Grid Search](https://goo.gl/CqfMg4)). Bear in mind that those features have been selected using a random forest algorithm, which vastly differs from the method presented here. There might therefore be alternative ways of selecting a subset of features that would yield a better outcome... The result of the backwards stepwise algorithm is depicted in the graph presented at [Kaggle's Titanic Toy Problem Revisited](https://goo.gl/E6F4bm#feature-selection). 


```{r include=FALSE}
predictors <- c("Title", "FarePred", "EmbarkedQorS", "TicketNum", "FamilyMembers", "Pclass")  

colToRemove <- setdiff (names(X), c(response, predictors, unselect))
for (col in colToRemove) {
  X[[col]] <- NULL
  Xt[[col]] <- NULL
}
trainUnsupervised <- trainUnsupervised[c(response, predictors)]
trainSupervised <- trainSupervised[c(response, predictors)]

```

# Autoencoder (Diabolo Network): Unsupervised Learning

We take advantage of the entire data set. In order to find adequately performing autoencoders, we rely on a grid search.

```{r echo=TRUE, warning=FALSE, results='hide', message=FALSE}
hyperParamsAutoencoder = list( 
  hidden = list(c(11, 4, 11), c(10, 4, 10), c(9, 5, 9), c(9, 4, 9), 
                c(7, 4, 7), c(8, 5, 8), c(8, 4, 8), c(8, 3, 8), c(7, 3, 7)),
  activation = c("Tanh") 
)

gridAutoencoder <- h2o.grid(
  x = predictors,
  autoencoder = TRUE,
  training_frame = trainUnsupervised,
  hyper_params = hyperParamsAutoencoder,
  search_criteria = list(strategy = "Cartesian"),
  algorithm = "deeplearning",
  grid_id = "grid_autoencoder", 
  reproducible = TRUE, 
  seed = 1,
  variable_importances = TRUE,
  categorical_encoding = "AUTO",
  score_interval = 10,
  epochs = 800,
  adaptive_rate = TRUE,
  standardize = TRUE,
  ignore_const_cols = FALSE)
```

The following table summarizes the grid results (it is sorted increasingly by 'mse'):
```{r echo=FALSE}
sortedGridAutoencoder <- h2o.getGrid("grid_autoencoder", sort_by = "mse", decreasing = FALSE)
tmpDf <- as.data.frame(sortedGridAutoencoder@summary_table)
knitr::kable(head(tmpDf[, -grep("model_ids", colnames(tmpDf))]), row.names = TRUE)
```

```{r include=FALSE}
bestAutoencoder <- h2o.getModel(sortedGridAutoencoder@model_ids[[1]])

bestAutoencoderErr <- as.data.frame(h2o.anomaly(bestAutoencoder, 
                                                trainUnsupervised, 
                                                per_feature = FALSE))

```

Considering the "best" autoencoder (i.e., the one with the lowest 'mse', which is the one with the hidden layers [`r bestAutoencoder@parameters$hidden`]), the two following figures illustrate the fact that it performs rather well; only a limited portion of the input signal could not be reconstructed. 

```{r  echo=FALSE, warning=FALSE, results='hide', message=FALSE, fig.width=10, fig.height=3.5}
plotReconstructionError <- function (error) {
  cut <- 0.5 * sd(error)
  sortedErr <- sort(error)
  sortedErrFrame <- data.frame (index = seq(0, length(sortedErr)-1), error = sortedErr)
  ylim <- c(min(sortedErrFrame$error), max(sortedErrFrame$error))
  xlim <- c(min(sortedErrFrame$index), max(sortedErrFrame$index))
  # could do as in https://stackoverflow.com/questions/11838278/plot-with-conditional-colors-based-on-values-in-r
  plot (x = sortedErrFrame$index[which(sortedErrFrame$error <= cut)], 
        y = sortedErrFrame$error[which(sortedErrFrame$error <= cut)], 
        type = "o", col="forestgreen", lwd=1, ylim = ylim, xlim = xlim, ylab="mse",
        main = "Reconstruction Error",
        xlab = "Sorted Index")
  par(new=TRUE)
  plot (x = sortedErrFrame$index[which(sortedErrFrame$error > cut)], 
        y = sortedErrFrame$error[which(sortedErrFrame$error > cut)],  
        type = "o", col="firebrick3", xaxt='n', yaxt='n', ann=FALSE, ylim = ylim, xlim = xlim, xlab="")
  return (0)
}

#opar <- par(mfrow = c(1, 2))
layout(matrix(c(1,2,2), 1, 3, byrow = TRUE))
plotReconstructionError (bestAutoencoderErr$Reconstruction.MSE)

# https://stackoverflow.com/questions/21858394/partially-color-histogram-in-r
h <- hist(x = bestAutoencoderErr$Reconstruction.MSE, breaks = 100, plot = FALSE)
cuts <- cut(h$breaks, c(-Inf, 0.5 * sd(bestAutoencoderErr$Reconstruction.MSE), Inf))
plot(h, col = c("forestgreen","firebrick3")[cuts], main = "Reconstruction Error", xlab = "mse", lty="blank")
#par(opar)
```

# Pretrained Neural Networks: Supervised Learning

Now we use the labaled data set (`train`) to turn the autoencoders generated above into classifiers. 

```{r  echo=TRUE, warning=FALSE, results='hide', message=FALSE}
getDlClassifier <- function (autoencoder, predictors, response, trainSupervised) {
  
  dlSupervisedModel <- h2o.deeplearning(
    y = response, x = predictors,
    training_frame = trainSupervised, 
    pretrained_autoencoder = autoencoder@model_id,
    reproducible = TRUE, 
    balance_classes = TRUE, 
    ignore_const_cols = FALSE,
    seed = 1,
    hidden = autoencoder@parameters$hidden, 
    categorical_encoding = "AUTO",
    epochs = autoencoder@parameters$epochs, 
    standardize = TRUE,
    activation = autoencoder@parameters$activation)
  
  return (dlSupervisedModel)
}
```

```{r include=FALSE}

# sequence of thresholds 
kThresholdsSequence <- seq(0.4, 0.6, 0.1)

count <- 1
models <- c()
modelThresholds <- c()
modelAccs <- c()
modelPretrainMse <- c()
modelHidden <- list()
testPreds <- list()
for (i in 1:length(sortedGridAutoencoder@model_ids)) {

  autoencoder <- h2o.getModel(sortedGridAutoencoder@model_ids[[i]])
  dlSupervisedModel <- getDlClassifier (autoencoder, predictors, response, trainSupervised)
  
  pred <- h2o.predict(object = dlSupervisedModel, newdata = validSupervised)
  
  # try to find an adequate threshold
  finalAcc <- 0
  finalTh <- 0
  for (th in kThresholdsSequence) {
    a <- mean (as.vector(ifelse(pred$p1 < th, 0, 1)) 
               == as.numeric(as.vector(validSupervised[, response]))) 
    if (finalAcc < a) {
      finalAcc <- a
      finalTh <- th
    }
  }

  models <- c(models, dlSupervisedModel)
  modelThresholds <- c(modelThresholds, finalTh)
  modelAccs <- c(modelAccs, finalAcc)
  modelPretrainMse <- c(modelPretrainMse, h2o.mse(autoencoder))
  modelHidden[[paste0(count)]] <- autoencoder@parameters$hidden
  
  predTest <- h2o.predict(object = dlSupervisedModel, newdata = test)
  testPreds[[paste0(count)]] <- as.vector(ifelse(predTest$p1 < modelThresholds[count], 0, 1))
  
  count <- count + 1
}
```

```{r include=FALSE}
dfResults <- data.frame (validAcc = modelAccs, 
                         Threshold = modelThresholds, 
                         HiddenLayers = unlist(lapply (modelHidden, function (x) paste0("(", paste(x, collapse=", "), ")"))),
                         PretrainMse = modelPretrainMse,
                         index = seq(1, length (modelAccs), 1))

# sort the results
ordering <- order(dfResults$validAcc, -dfResults$PretrainMse, decreasing = TRUE)
dfResults <- dfResults[ordering, ]
```

# Results

Here is the *head* of the sorted grid result---decreasingly by validAcc (accuracy achieved on the validation set); then, increasingly by pre-train 'mse'. The column *index* is just a way for us to retrieve the prediction associated with the row.

```{r echo=FALSE}
knitr::kable(head(dfResults), row.names = FALSE)
```

```{r include=FALSE}
testPreds[[dfResults[1, ]$index]]
```

Submitting the **first row's model** (index `r dfResults[1, ]$index`) yielded the accuracy of **0.80383** on Kaggle.

```{r include=FALSE}
# save the best result
tryCatch({
  pathExt <- paste0("-", Sys.Date(), ".csv")
  saveOutPutCsv (Xt$PassengerId, testPreds[[dfResults[1, ]$index]], 
                 file.path(".", paste0("result", pathExt), 
                           fsep = .Platform$file.sep) )
}, error = function(e) {
  print(e)
})
```
