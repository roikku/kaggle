---
title: 'Preliminary Investigation: PCA & Boosting'
author: 'Loic Merckel'
date: '30 June 2017'
output:
  html_document:
    number_sections: false
    toc: true
    highlight: tango
    theme: cosmo
    smart: true
---

<style type="text/css">
  h1.title { font-weight: bold; } h1 { font-weight: normal; } .author { font-weight: normal; font-size: 1.5em; }
</style>


```{r include=FALSE}
# License -----------------

# Copyright 2017 Loic Merckel
# 
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
# http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```


We present a preliminary approach for dimensionality reduction reposing on principal component analysis. We first train a gradient boosting machine (GBM) using the entire set of features (baseline); then we attempt to improve the baseline result by reducing the feature dimensionality (a doing motivated by the fact that the number of features is large compared to the number of rows). 

It turned out that our attempt was in vain; we found that using principal components as features did not yield a good outcome.

The feature tinkering part might be overly shallow, we probably need to further the initial preparation of features before moving on with training machines (as we say, *garbage in, garbage out*...).

Furthermore, we have only limited computational resources (a MacBook), and therefore, an extensive grid search was not really realistic. It is thus plausible that our result could be improved (to a certain extent) simply with more parameters tuning.


```{r include=FALSE}
pkgs <- c("corrplot", "car", "heplots", "plyr", "caret",
          "dummies", "randomForest", "xgboost", "parallel", "Metrics") ;
for (pkg in pkgs) {
  if (! (pkg %in% rownames(installed.packages()))) { install.packages(pkg) }
  require(pkg, character.only = TRUE)
}
rm(pkgs)
rm(pkg)
```

```{r include=FALSE}
# data ---------------------------
rm(list=ls(all=TRUE))

getDataSet <- function (filePath) {
  if(!file.exists(filePath)){
    stop ("Error: the file cannot be found...")
  }
  if (grepl('zip$', filePath)) {
    tmp <- unzip(filePath, exdir = dirname(filePath))
    data <- read.csv(tmp, header = TRUE)
    file.remove(tmp)
    return (data)
  } else {
    return (read.csv(filePath, header = TRUE))
  }
}

isOnKaggle <- FALSE
if (isOnKaggle) {
  X <- getDataSet (file.path("..", "input", "train.csv", fsep = .Platform$file.sep))
  Xt <- getDataSet(file.path("..", "input", "test.csv", fsep = .Platform$file.sep))
} else {
  X <- getDataSet (file.path(".", "data", "train.csv", fsep = .Platform$file.sep))
  Xt <- getDataSet(file.path(".", "data", "test.csv", fsep = .Platform$file.sep))
}

Xt$y <- NA

# move the response to the end
X <- X[, c(setdiff(names(X),"y"), "y")]
Xt <- Xt[, c(setdiff(names(Xt),"y"), "y")]
```

```{r include=FALSE}
# Settings ------------

kProportionExplainedVariance <- 0.9

kOutputFolder <- file.path(".", fsep = .Platform$file.sep) 
```

```{r include=FALSE}
saveOutPutCsv <- function (id, pred, file) {
  res <- data.frame(Id=id)
  res[["y"]] <- as.numeric(as.vector(pred))
  write.csv(res, file=file, row.names = FALSE,  quote = FALSE)
}

isConstant <- function (x) {
  return(all(duplicated(x)[-1L]))
}

getFormula <- function (response, features, factorResponse = TRUE) {
  if (factorResponse) {
    formulaStr <- paste0("factor(", response, ") ~ ")
  } else {
    formulaStr <- paste0(response, " ~ ")
  }
  first <- TRUE
  for (str in features) {
    if (first) {
      first <- FALSE
      formulaStr <- paste0(formulaStr, str)
    } else {
      formulaStr <- paste0(formulaStr, " + ", str)
    }
  }
  return (as.formula(formulaStr))
}

rsquared <- function (y, predicted){
  return (1 - sum((y-predicted)^2)/sum((y-mean(y))^2))
}

convertInteger2Numeric <- function (data) {
  
  for (c in names (data)) {
    if (is.integer(data[[c]])) {
      data[[c]] <- as.numeric(data[[c]])
    }
  }
  return (data)
}

saveResult <- function (id, result, prefix) {
  pathExt <- paste0("-", Sys.Date(), ".csv")
  tryCatch({
    
    if(!file.exists(kOutputFolder)){
      dir.create(kOutputFolder, showWarnings = FALSE, recursive = TRUE)
    }
    
    path <- file.path(kOutputFolder, 
                      paste0(prefix, pathExt), 
                      fsep = .Platform$file.sep)
    
    saveOutPutCsv (id, result, path)
  }, error = function(e) {
    print(e)
  })
  return (0)
}
```

# Features Tinkering

There are `r ncol(X)` features and only `r nrow(X)` rows in the training set. We might be interested in reducing the number of features.

Since the features have been anonymized, it is it rather unclear what those features are. A rapid analysis reveals no missing values; besides, many columns appear to be *quasi*-constant.

```{r include=FALSE}
toIgnore <- c("ID", "y")
numVal <- c()
nonNumVal <- c()
for (feature in colnames(X)) {
  if (feature %in% toIgnore) {
    next
  }
  if (class(X[[feature]]) == "integer") {
    if (sd(X[[feature]]) == 0) {
      toIgnore <- c(toIgnore, feature)
    } else {
      numVal <- c(feature, numVal)
    }
  } else {
    nonNumVal <- c(feature, nonNumVal)
  }
}
```

There are `r length(nonNumVal)` categorical variables, and `r length(numVal)` numerical variables with non-null standard deviation (those numerical values are either zero or one). The following graph reveals some correlation between the numerical features.

```{r  echo=FALSE, warning=FALSE, results='hide', message=FALSE, fig.width=9, fig.height=9}

set.seed(1)
toIgnore <- c("ID", "y")
nrowx <- 4
nrowy <- 4
opar <- par(mfrow = c(nrowy, nrowx))
for(i in 1:(nrowx * nrowy)) {
  randomSample <- sample(numVal, 10, replace = FALSE, prob = NULL)
  
  correlations <- cor(X[, setdiff(randomSample, toIgnore)], use="everything")
  corrplot(correlations, method="circle", type="lower",  sig.level = 0.01, insig = "blank", tl.cex = 0.7)
}
par(opar)
```



## Convert Categorical Variables to Numerical

We use one-hot encoding.

```{r echo=FALSE, , warning=FALSE, message=FALSE}
fds <- rbind(X, Xt)
numFds <- dummy.data.frame(
  fds, 
  names = nonNumVal, verbose = TRUE)

numX <- numFds[1:nrow(X), ]
numXt <- numFds[(nrow(X)+1):(nrow(X)+nrow(Xt)), ]
```

## Remove Constant Columns

```{r include=FALSE}
toRm <- c()
for (c in names(numX)) {
  if (isConstant(numX[[c]])) {
    toRm <- c(toRm, c)
  }
}

for (feature in toRm) {
  numX[[feature]] <- NULL
  numXt[[feature]] <- NULL
}
```

The features `r paste(toRm, collapse=", ")` are constant in the training set, and thus dropped.


## Remove Outlier from Response[^1]

[^1]: This section has been added to the [initial version of this post](https://goo.gl/uM1FVS); and has been taken from our subsequent kernels [Greedy Selection & Boosting](https://goo.gl/NmKiu1).

```{r  echo=FALSE, warning=FALSE, results='hide', message=FALSE, fig.width=9, fig.height=5}
# with outlier
opar <- par(fig=c(0, 0.5, 0, 0.8))
hist(x = numX$y, breaks = 100, col = "forestgreen", plot = TRUE, xlab = "Response (time)", 
     main = NULL)
par(fig=c(0, 0.5, 0.50, 1), new = TRUE)
boxplot(numX$y, col = "forestgreen", outcol="firebrick4", horizontal = TRUE, axes = FALSE)
mtext("With the clear outlier", outer = FALSE, line=0)

# without outlier
par(fig=c(0.5, 1, 0, 0.8), new = TRUE)
hist(x = numX[-which(numX$y > 250), ]$y, breaks = 100, col = "forestgreen", plot = TRUE, xlab = "Response (time)", 
     main = NULL)
par(fig=c(0.5, 1, 0.50, 1), new = TRUE)
boxplot(numX[-which(numX$y > 250), ]$y, col = "forestgreen", outcol="firebrick4", horizontal = TRUE, axes = FALSE)
mtext("Without the outlier", outer = FALSE, line=0)
title("Histogram of Response", outer = TRUE, line=-2)
par (opar)
```


The figure above (left) reveals a clear outlier with a response time above 250. Let's remove that likely spurious row. 

```{r echo=TRUE, warning=FALSE, results='hide', message=FALSE}
numX <- numX[-which(numX$y > 250), ]
```

The right chart shows the resulting distribution. 

## Examine Possible Duplicated Rows

```{r echo=TRUE, warning=FALSE, results='hide', message=FALSE}
duplicatedRows <- duplicated(numX[, setdiff(names(numX), c("y", "ID"))])
```

There are `r sum(duplicatedRows)` rows with identical features (except for the `ID` and the response).

```{r echo=TRUE, warning=FALSE, results='hide', message=FALSE}
formula <- getFormula (
  response = "y", 
  features = setdiff(names(numX), c("y", "ID")),
  factorResponse = FALSE)

index <- 1
duplicatedRowResponse <- list()
aggNumX <- aggregate(formula, data = numX, drop = TRUE, 
                     FUN = function (ys) {
                       if (length(ys) > 1) {
                         duplicatedRowResponse[[paste0(index)]] <- ys
                         duplicatedRowResponse <<- duplicatedRowResponse
                         index <<- index + 1
                       }
                       return(mean(ys))
                     })
```

It is interesting to observe that with the exact same configuration the testing times differ (and quite significantly for certain configurations---as depicted by the figure below). This is perhaps due to imperfection of the measuring device or perhaps human factors (tests not fully automated).

```{r  echo=FALSE, warning=FALSE, results='hide', message=FALSE, fig.width=9, fig.height=9}
opar <- par(mfrow = c(3, 1), 
            oma = c(5, 4, 2, 0) + 0.1,
            mar = c(2.5, 0, 1, 1) + 0.1)
for (i in 1:3) {
  boxplot(duplicatedRowResponse[((i-1) * floor(length(duplicatedRowResponse)/3 + 1)):(i * floor(length(duplicatedRowResponse)/3))], 
          ylab="Testing Time",
          horizontal = FALSE,
          col = "forestgreen")
  if (i == 1) {
    title(main = "Distribution of Testing Times for Identical Configurations",
          xlab="Configuration Index",
          outer = TRUE, line = 0)
  }
}
par(opar)
```


The problem for our regression model is that such discrepancies might confuse the machine... (especially the number of rows is not that large...)
  
Let's aggregate those duplicated rows by identical features and set the response to the mean of their responses. Doing so, the already small number of rows further diminishes, but we might still gain from it.
  
One can remark that the IDs are lost, but we do not need them on the training set. Let's add a dummy column `ID` so that all data sets are consistent.

```{r echo=TRUE, warning=FALSE, results='hide', message=FALSE}
  numX <- cbind(ID = 0, aggNumX) 
```


```{r include=FALSE}
unselect <- c("ID")
response <- "y"
predictors <- setdiff(names(numX), c(response, unselect)) 

train <- numX 
test <- numXt

train <- convertInteger2Numeric(train)
test <- convertInteger2Numeric(test)
```

# Baseline (Gradient Boosting Machine without Feature Reduction)

We try using the entire set of features.

The two following vectors of indexes to ignore should be identical given that `ID` is consistently the first column and `response` the last one.  

```{r echo=TRUE, warning=FALSE, results='hide', message=FALSE}
indexToIgnoreTrain <- c(grep(paste0("^", response, "$"), names(train)), 
                   grep("^ID$", names(train)))

indexToIgnoreTest <- c(grep(paste0("^", response, "$"), names(test)), 
                        grep("^ID$", names(test)))
```

Note that we rely on [XGBoost](http://xgboost.readthedocs.io/en/latest/) for the GBM training, in concert with the [caret package](http://topepo.github.io/caret/index.html) for both grid search and cross-validation. Due to severe limitations in computational resources, the grid only spans very narrow ranges for hyper-parameters (consequently, we might very well miss more adequate settings). The situation is even worse on Kaggle (a 3-folds cross-validation with a two-row grid reaches the time out; hence the only one row grid below---it is still useful for when the script is executed on a decent machine, it is easy to expend the grid).

```{r echo=TRUE, warning=FALSE, results='hide', message=FALSE}

xgbGrid <- expand.grid(
  nrounds = c(800),
  eta = c(0.01),
  max_depth = c(4), 
  gamma = 0, 
  colsample_bytree = c(0.8),  
  min_child_weight = c(3), 
  subsample = c(0.7)) 

xgbTrControl <- trainControl(
  method = "cv",
  number = ifelse(isOnKaggle, 3, 8),
  verboseIter = TRUE,
  returnResamp = "final",                                                       
  allowParallel = TRUE)

set.seed(1)
xgbBaseline <- train(
  x = data.matrix(train[, -indexToIgnoreTrain]),
  y = train[[response]],
  trControl = xgbTrControl,
  tuneGrid = xgbGrid,
  method = "xgbTree",
  metric = "Rsquared") 
#,nthread = parallel:::detectCores())
```

Summary of the training:
```{r echo=FALSE, warning=FALSE, message=FALSE}
xgbBaseline
```

A more extensive grid search could lead to improved performance, but we are quite limited in computational resources...

We can submit to Kaggle's LB to see how well the baseline model performs on the test set.

```{r echo=TRUE, warning=FALSE, results='hide', message=FALSE}
resultBaseline <- predict(xgbBaseline, data.matrix(test[, -indexToIgnoreTest]))
saveResult(Xt$ID, resultBaseline, "result-baseline")
```

# Dimensionality Reduction 

## Principal Component Analysis (PCA)

We can include the test set into the PCA, but we must **bear in mind that it is definitely not a good practice** (to say it euphemistically), for the test set becomes part of the training process (basically, the resulting model might perform well on this test set, but might miserably fail to generalize to a new unseen data set). Here we do it because we won't have any new data...


```{r echo=TRUE, warning=FALSE, results='hide', message=FALSE}
pcaTrain <- rbind(
  train[, predictors], 
  test[, predictors])
```

```{r include=FALSE}
normalize <- (max(apply(pcaTrain, 2, max)) - min(apply(pcaTrain, 2, min))) > 1

set.seed(1)
```

```{r echo=TRUE, warning=FALSE, results='hide', message=FALSE}
pC <- prcomp(pcaTrain, center = normalize, scale. = normalize)
```
```{r include=FALSE}
if (FALSE) {
  biplot(pC, scale = 0)
}
```
```{r echo=TRUE, warning=FALSE, results='hide', message=FALSE}
stdDev <- pC$sdev
propotionOfVarianceExplained <- stdDev^2 /sum(stdDev^2)
```
  
```{r  echo=FALSE, warning=FALSE, results='hide', message=FALSE, fig.width=8, fig.height=6}
colThrs <- kProportionExplainedVariance
plot(cumsum(propotionOfVarianceExplained), xlab = "Principal Component Index",
     ylab = "Cumulative Proportion of Variance Explained",
     pch = 20,
     cex = 2.0,
     col = ifelse(cumsum(propotionOfVarianceExplained) < colThrs, "forestgreen", "firebrick3"))

# get the number of components that explain at least kProportionExplainedVariance
# of the variance
cumSumPropVarExpl <- cumsum(propotionOfVarianceExplained)
numberOfComponents <- 1
while (cumSumPropVarExpl[numberOfComponents] < kProportionExplainedVariance) {
  numberOfComponents <- numberOfComponents + 1
}
```  

The first **`r numberOfComponents` components** explained **`r kProportionExplainedVariance * 100`%** of the variance in the data.
  
  
## Gradient Boosting Machine


```{r echo=TRUE, warning=FALSE, results='hide', message=FALSE}
trainReduced <- as.data.frame(predict(pC, newdata = train[, predictors]))
testReduced <- as.data.frame(predict(pC, newdata = test[, predictors]))
```

We select the `r numberOfComponents` first components.

```{r echo=TRUE, warning=FALSE, results='hide', message=FALSE}
trainReduced <- trainReduced[, 1:numberOfComponents]
testReduced <- testReduced[, 1:numberOfComponents]
```
  
We can verify visually that there is only very few correlations between those components.
  
```{r  echo=FALSE, warning=FALSE, results='hide', message=FALSE, fig.width=9, fig.height=9}
tryCatch({
  set.seed(1)
  nrowx <- 1
  nrowy <- 1
  opar <- par(mfrow = c(nrowy, nrowx))
  for(i in 1:(nrowx * nrowy)) {
    randomSample <- sample(names(trainReduced[, 1:numberOfComponents]), 
                           min(50, numberOfComponents), replace = FALSE, 
                           prob = NULL)

    correlations <- cor(trainReduced[, randomSample], use="everything")
    corrplot(correlations, method="circle", type="lower",  sig.level = 0.01, insig = "blank", tl.cex = 0.7)
  }
  par(opar)
}, error = function(e) {
  print (e)
})  
```  

Add the column to predict.
```{r echo=TRUE, warning=FALSE, results='hide', message=FALSE}
trainReduced[[response]] <- train[[response]]
testReduced[[response]] <- NA
```

```{r echo=TRUE, warning=FALSE, results='hide', message=FALSE}

```

```{r include=FALSE}
indexToIgnoreTrain <- c(grep(paste0("^", response, "$"), names(trainReduced)), 
                        grep("^ID$", names(trainReduced)))

indexToIgnoreTest <- c(grep(paste0("^", response, "$"), names(testReduced)), 
                        grep("^ID$", names(testReduced)))
seed <- 1
set.seed(seed)
```

We train the GBM.

```{r echo=TRUE, warning=FALSE, results='hide', message=FALSE}
xgbGrid <- expand.grid(
  nrounds = c(400),
  eta = c(0.01),
  max_depth = c(6, 8), 
  gamma = 0, 
  colsample_bytree = c(0.8),  
  min_child_weight = c(3), 
  subsample = c(0.7)) 

xgbPca <- train(
  x = data.matrix(trainReduced[, -indexToIgnoreTrain]),
  y = trainReduced[[response]],
  trControl = xgbTrControl,
  tuneGrid = xgbGrid,
  method = "xgbTree",
  metric = "Rsquared") 
  #,nthread = parallel:::detectCores())
```

Summary of the training:
```{r echo=FALSE, warning=FALSE, message=FALSE}
xgbPca
```

Oops! Quite disenchanting outcome...

Predict the testing time on the test set---although, judging from the reported performance, it might not be necessary to submit anything out of that model to the Kaggle's LB... The baseline result should be less embarrassing... (We got 0.55383 with a variant of the baseline case.)

```{r echo=TRUE, warning=FALSE, results='hide', message=FALSE}
resultPca <- predict(xgbPca, data.matrix(testReduced[, -indexToIgnoreTest]))
saveResult(Xt$ID, resultPca, "result-pca")
```

# Concluding Remarks

It is a bit disappointing to observe no improvement after reducing the number of features using PCA; and even worse, we get some noticeable degradation of the baseline result. Sometimes things like that happen... A more thorough feature tinkering might help here to reduce the number of features and achieve better performance!
