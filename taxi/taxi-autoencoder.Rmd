---
title: 'Deep Semi-supervised Learning'
author: 'Loic Merckel'
date: '28 July 2017'
output:
  html_document:
    number_sections: false
    toc: true
    highlight: tango
    theme: cosmo
    smart: true
---

<style type="text/css">
  h1.title { font-weight: bold; } h1 { font-weight: normal; } .author { font-weight: normal; font-size: 1.5em; }
</style>


```{r include=FALSE}
# License -----------------

# Copyright 2017 Loic Merckel
# 
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
# http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```


Throughout this kernel we show how to:

- Leverage unlabeled data in a supervised regression problem (a technique often referred to as **semi-supervised learning**).
- Use the autoencoder and deeplearning capability of the  [H<small>2</small>O](https://www.h2o.ai/) framework.
 
Note that the method presented in this page does not propose a model that can make predictions from new data. The test set is extensively used during the process; it can be regarded as a label reconstruction method.

This kernel is a complement of our previous kernels: *[Autoencoder and Deep Features](https://goo.gl/KAwTiR)*.


```{r include=FALSE}
pkgs <- c("corrplot", "caret", "data.table", "plyr",
          "xgboost", "parallel", "Metrics", "maps", 
          "ggmap", "lubridate", "fasttime", "gridExtra", 
          "geosphere") 
for (pkg in pkgs) {
  if (! (pkg %in% rownames(installed.packages()))) { install.packages(pkg) }
  require(pkg, character.only = TRUE)
}
rm(pkgs)
rm(pkg)
```

```{r include=FALSE}
# data ---------------------------
rm(list=ls(all=TRUE))

kIsOnKaggle <- FALSE
if (kIsOnKaggle) {
  X <- fread(file.path("..", "input", "train.csv", fsep = .Platform$file.sep), 
             header = TRUE, data.table = TRUE, na.strings=c("NA","?", ""))
  Xt <- fread(file.path("..", "input", "test.csv", fsep = .Platform$file.sep), 
             header = TRUE, data.table = TRUE, na.strings=c("NA","?", ""))
} else {
  X <- fread(file.path(".", "data", "train.csv", fsep = .Platform$file.sep), 
             header = TRUE, data.table = TRUE, na.strings=c("NA","?", ""))
  Xt <-  fread(file.path(".", "data", "test.csv", fsep = .Platform$file.sep), 
             header = TRUE, data.table = TRUE, na.strings=c("NA","?", ""))
}

Xt[, trip_duration := NA]
class(Xt$trip_duration) <- class(X$trip_duration)
```

```{r include=FALSE}
isConstant <- function (x) {
  return(all(duplicated(x)[-1L]))
}
```

# Features Tinkering

This part is identical to the one of the same name in the previous kernel *[Autoencoder and Deep Features](https://goo.gl/KAwTiR)*.

```{r include=FALSE}
X[, dropoff_datetime := NULL]
```

```{r include=FALSE}
toIgnore <- c("id", "trip_duration")
```

```{r include=FALSE}
X <- X[-which(X$pickup_longitude > -74
             & X$pickup_latitude < 40.57), ]
```


```{r include=FALSE}
# outliers (Hampel's test)
maxThreshold <- 3 * 1.48 * mad(X$trip_duration, na.rm = TRUE) 
                    + median(X$trip_duration, na.rm = TRUE)  
```

```{r include=FALSE}
X <- X[-which(X$trip_duration > maxThreshold), ]
```

```{r include=FALSE}
rm(maxThreshold)
```

```{r include=FALSE}
X[, distance_as_the_crow_flies := distHaversine(data.table(pickup_longitude, pickup_latitude),
              data.table(dropoff_longitude, dropoff_latitude))]
                  
Xt[, distance_as_the_crow_flies := distHaversine(data.table(pickup_longitude, pickup_latitude),
              data.table(dropoff_longitude, dropoff_latitude))]
```                  

```{r include=FALSE}
X[, pickup_datetime := fastPOSIXct (pickup_datetime, tz = "EST")]
X[, wday := wday(pickup_datetime)]
X[, hour := hour (pickup_datetime)]

Xt[, pickup_datetime := fastPOSIXct (pickup_datetime, tz = "EST")]
Xt[, wday := wday(pickup_datetime)]
Xt[, hour := hour (pickup_datetime)]
```

```{r include=FALSE}
toIgnore <- c(toIgnore, c("pickup_datetime"))
```

```{r include=FALSE}
numVal <- c()
nonNumVal <- c()
for (feature in colnames(X)) {
  if (feature %in% toIgnore) {
    next
  }
  if (class(X[[feature]]) %in% c("integer", "numeric")) {
    if (sd(X[[feature]]) == 0) {
      toIgnore <- c(toIgnore, feature)
    } else {
      numVal <- c(feature, numVal)
    }
  } else {
    nonNumVal <- c(feature, nonNumVal)
  }
}
rm(feature)
```

```{r include=FALSE}
X[, store_and_fwd_flag := ifelse (store_and_fwd_flag == "N", 0, 1)]
```

```{r include=FALSE}
Xt[, store_and_fwd_flag := ifelse (store_and_fwd_flag == "N", 0, 1)]
```


# Data Split

```{r include=FALSE}
if (! ("h2o" %in% rownames(installed.packages()))) { install.packages("h2o") }
require("h2o")

tryCatch(
  if (h2o.clusterIsUp()) {
    h2o.shutdown(prompt=FALSE)
    Sys.sleep(5)
  }, error = function(e) {
    
  })

h2o.init(nthreads = parallel:::detectCores(), 
         max_mem_size = "15g", min_mem_size = "1g")
h2o.removeAll()
h2o.clusterStatus()
```

```{r include=FALSE}
# We remove the column pickup_datetime because H2O does not handle POSIXct type, 
# and we do not really need this column any longer (wday and hour are used instead)

X[, pickup_datetime := NULL]
Xt[, pickup_datetime := NULL]
```

```{r echo=TRUE, warning=FALSE, results='hide', message=FALSE}
if (kIsOnKaggle) {
  # we reduce the size of both the training and testing set to avoid time out
  set.seed(1)
  trainIndexes <- createDataPartition(
    y = X[["trip_duration"]], p = 0.15, list = FALSE) 
  train <- as.h2o(X[trainIndexes, ])
  rm(trainIndexes)
  set.seed(1)
  Xt[, trip_duration := 0]
  test <- as.h2o(Xt[sample(seq(1, nrow(Xt)), 0.25 * nrow(Xt)), ])
} else {
  train <- as.h2o(X)
  test <- as.h2o(Xt[, trip_duration := 0])
}
```

The entire data set is used for training autoencoders (unsupervised learning).

```{r echo=TRUE, warning=FALSE, results='hide', message=FALSE}
trainUnsupervised <- h2o.rbind(train, test) 
```

Regarding the deep learning regressor, a validation set is extracted from the train set. We use a validation set to rank the different models (obtained from the autoencoder grid search). 

```{r echo=TRUE, warning=FALSE, results='hide', message=FALSE}
splits <- h2o.splitFrame(
  data = train, 
  ratios = 0.7,  
  seed = 1
)
trainSupervised <- splits[[1]]
validSupervised <- splits[[2]]
```


```{r include=FALSE}
predictors <- setdiff (names(X), c(toIgnore))
```


# Diabolo Network: Unsupervised Learning

```{r echo=TRUE, warning=FALSE, results='hide', message=FALSE}
if (kIsOnKaggle) {
  hyperParamsAutoencoder = list( 
    hidden = list(c(9, 5, 9), c(11, 8, 11), c(10, 5, 10), c(8, 4, 8)),
    activation = c("Tanh"))
} else {
  hyperParamsAutoencoder = list( 
    hidden = list(c(11, 8, 11), c(10, 8, 10), c(9, 5, 9), c(8, 5, 8), 
                  c(7, 5, 7), c(6, 5, 6), c(11, 7, 11), c(10, 7, 10), 
                  c(9, 4, 9), c(8, 4, 8), c(7, 4, 7), c(6, 4, 6), 
                  c(11, 6, 11), c(10, 6, 10), c(9, 3, 9), c(8, 3, 8), 
                  c(7, 3, 7), c(6, 3, 6), c(11, 5, 11), c(10, 5, 10),
                  c(11, 4, 11), c(10, 4, 10), c(11, 8, 5, 8, 11)),
    activation = c("Tanh"))
}

h2o.rm("grid_autoencoder")
gridAutoencoder <- h2o.grid(
  x = predictors,
  autoencoder = TRUE,
  training_frame = trainUnsupervised,
  hyper_params = hyperParamsAutoencoder,
  search_criteria = list(strategy = "Cartesian"),
  algorithm = "deeplearning",
  grid_id = "grid_autoencoder", 
  reproducible = TRUE, 
  seed = 1,
  variable_importances = FALSE,
  categorical_encoding = "AUTO",
  score_interval = 10,
  epochs = 800,
  adaptive_rate = TRUE,
  standardize = TRUE,
  ignore_const_cols = FALSE)
```

```{r include=FALSE}
rm (hyperParamsAutoencoder)
```

The following table summarizes the grid results (it is sorted increasingly by 'mse'):
```{r echo=FALSE}
sortedGridAutoencoder <- h2o.getGrid("grid_autoencoder", 
                                     sort_by = "mse", decreasing = FALSE)
tmpDf <- as.data.frame(sortedGridAutoencoder@summary_table)
knitr::kable(head(tmpDf[, -grep("model_ids", colnames(tmpDf))]), row.names = TRUE)
rm(tmpDf)
```

```{r include=FALSE}
bestAutoencoder <- h2o.getModel(sortedGridAutoencoder@model_ids[[1]])

bestAutoencoderErr <- as.data.frame(h2o.anomaly(bestAutoencoder, 
                                                train, 
                                                per_feature = FALSE))
```


## Reconstruction Error

Considering the "best" autoencoder (i.e., the one with the lowest 'mse', which is the one with the hidden layers [`r bestAutoencoder@parameters$hidden`]), the two following figures illustrate the fact that it performs rather well; only a limited portion of the input signal could not be reconstructed. 

```{r  echo=FALSE, warning=FALSE, results='hide', message=FALSE, fig.width=10, fig.height=3.5}
plotReconstructionError <- function (error) {
  cut <- 0.5 * sd(error)
  sortedErr <- sort(error)
  sortedErrFrame <- data.frame (index = seq(0, length(sortedErr)-1), error = sortedErr)
  ylim <- c(min(sortedErrFrame$error), max(sortedErrFrame$error))
  xlim <- c(min(sortedErrFrame$index), max(sortedErrFrame$index))
  # could do as in https://stackoverflow.com/questions/11838278/plot-with-conditional-colors-based-on-values-in-r
  plot (x = sortedErrFrame$index[which(sortedErrFrame$error <= cut)], 
        y = sortedErrFrame$error[which(sortedErrFrame$error <= cut)], 
        type = "o", col="forestgreen", lwd=1, ylim = ylim, xlim = xlim, ylab="mse",
        main = "Reconstruction Error",
        xlab = "Sorted Index")
  par(new=TRUE)
  plot (x = sortedErrFrame$index[which(sortedErrFrame$error > cut)], 
        y = sortedErrFrame$error[which(sortedErrFrame$error > cut)],  
        type = "o", col="firebrick3", xaxt='n', yaxt='n', ann=FALSE, ylim = ylim, xlim = xlim, xlab="")
  return (0)
}

layout(matrix(c(1,2,2), 1, 3, byrow = TRUE))
plotReconstructionError (bestAutoencoderErr$Reconstruction.MSE)

# https://stackoverflow.com/questions/21858394/partially-color-histogram-in-r
h <- hist(x = bestAutoencoderErr$Reconstruction.MSE, breaks = 100, plot = FALSE)
cuts <- cut(h$breaks, c(-Inf, 0.5 * sd(bestAutoencoderErr$Reconstruction.MSE), Inf))
plot(h, col = c("forestgreen","firebrick3")[cuts], main = "Reconstruction Error", xlab = "mse", lty="blank")
rm(h, cuts)
```


## Deep Features Visualization

```{r include=FALSE}
layer <- 2
autoencoder <- bestAutoencoder #h2o.getModel("grid_autoencoder_model_0")
```

The kernel *[Autoencoder and Deep Features](https://goo.gl/KAwTiR)* describes how one could visualize deep features. Here we show some views from the second layer of the network with the layers [`r autoencoder@parameters$hidden`].

```{r  include=FALSE}
plotDeepFeatures <- function(data, maxPlot = 16, ncol = 4) {
  count <- 1
  plotList <- list()
  n <- (ncol(data) - 1)
  for (i in 1:(n-1)) {
    for (j in (i+1):n) {
      plotList[[paste0("p", count)]] <- ggplot(
        data, 
        aes_string(
          x = paste0("DF.L", layer, ".C", i), 
          y = paste0("DF.L", layer, ".C", j), 
          color = "log_trip_duration")) +
        geom_point(alpha = 0.9, aes(colour = log_trip_duration)) +
        scale_colour_gradientn(colours = rev(rainbow(10)[1:8])) +
        theme(legend.position = 
                ifelse(count == min((n-1)*n, maxPlot), "right", "none")) +     
        labs(color="log(t)")
      
      count <- count + 1
      if (count > maxPlot) {
        break
      }
    }
    if (count > maxPlot) {
      break
    }
  }
  grid.arrange(grobs = as.list(plotList), ncol = ncol)
}
```


```{r  echo=FALSE, warning=FALSE, results='hide', message=FALSE, fig.width=7.5, fig.height=7.5}
deepFeature <- h2o.deepfeatures(autoencoder, train, layer = layer)
data <- as.data.frame(deepFeature)
data$log_trip_duration <- log(as.vector(train$trip_duration))
set.seed(1)
plotDeepFeatures(
  data[sample(seq(1, nrow(data)), min(10000, nrow(data))), ]
  , 9, 3)
rm (deepFeature, data)
```


# Pretrained Neural Networks: Supervised Learning

Now we use the labaled data set (`train`) to turn the autoencoders generated above into regressors. 

```{r  echo=TRUE, warning=FALSE, results='hide', message=FALSE}
getDlRegressor <- function (autoencoder, predictors, response, 
                            trainSupervised, validSupervised) {
  
  dlSupervisedModel <- h2o.deeplearning(
    y = response, x = predictors,
    training_frame = trainSupervised, 
    validation_frame = validSupervised,
    pretrained_autoencoder = autoencoder@model_id,
    reproducible = TRUE, 
    balance_classes = FALSE, 
    ignore_const_cols = FALSE,
    seed = 1,
    hidden = autoencoder@parameters$hidden, 
    categorical_encoding = "AUTO",
    epochs = autoencoder@parameters$epochs, 
    standardize = TRUE,
    activation = autoencoder@parameters$activation)
  
  return (dlSupervisedModel)
}
```


```{r include=FALSE}
count <- 1
models <- c()
modelRmsles <- c()
modelPretrainMses <- c()
modelHidden <- list()
testPreds <- list()
for (i in 1:length(sortedGridAutoencoder@model_ids)) {

  autoencoder <- h2o.getModel(sortedGridAutoencoder@model_ids[[i]])
  dlSupervisedModel <- getDlRegressor (autoencoder, 
                                       predictors, 
                                       "trip_duration", 
                                       trainSupervised, 
                                       validSupervised)
  
  pred <- h2o.predict(object = dlSupervisedModel, newdata = validSupervised)
  predVect <- as.vector(pred$predict)
  # we are predicting a duration, unlikely to be negative...
  # besides, rmsle would not like it...
  predVect[predVect < 0] <- 0
  
  models <- c(models, dlSupervisedModel)
  modelRmsles <- c(modelRmsles, 
                   rmsle(actual = as.vector(validSupervised[["trip_duration"]]), 
                         predicted = predVect))
  modelPretrainMses <- c(modelPretrainMses, h2o.mse(autoencoder))
  modelHidden[[paste0(count)]] <- autoencoder@parameters$hidden
  
  testPreds[[paste0(count)]] <- h2o.predict(object = dlSupervisedModel, newdata = test)
  
  count <- count + 1
}
```

```{r include=FALSE}
dfResults <- data.frame (validRmsle = modelRmsles, 
                         HiddenLayers = unlist(lapply (modelHidden, 
                             function (x) paste0("(", paste(x, collapse=", "), ")"))),
                         PretrainMse = modelPretrainMses,
                         index = seq(1, length (modelRmsles), 1))

# sort the results
ordering <- order(dfResults$validRmsle, dfResults$PretrainMse, decreasing = FALSE)
dfResults <- dfResults[ordering, ]
```


Here is the *head* of the sorted grid result---increasingly by validRmsle (RMSLE achieved on the validation set); then, by pre-train 'mse'. The column *index* is just a way for us to retrieve the prediction associated with the row.

```{r echo=FALSE}
knitr::kable(head(dfResults), row.names = FALSE)
```

We can average a few best models.

```{r echo=TRUE, warning=FALSE, results='hide', message=FALSE}
result <- rep(0, nrow(Xt))
n <- min(2, nrow(dfResults))
for (i in 1:n){
  result <- result + as.vector(testPreds[[dfResults[i, ]$index]])
}
result <- result / n
result[result < 0] <- 0
```

```{r echo=TRUE, warning=FALSE, results='hide', message=FALSE}
fwrite(data.table(id = Xt$id, trip_duration = result), 
       file = file.path(".", paste0("output-nn-", Sys.Date(), ".csv"), 
                        fsep = .Platform$file.sep), 
       row.names = FALSE,  quote = FALSE)
```
